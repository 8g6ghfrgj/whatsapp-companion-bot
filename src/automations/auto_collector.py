"""
ğŸ“¥ AutoCollector - Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø±ÙˆØ§Ø¨Ø·
"""

import asyncio
import logging
import re
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Set
from urllib.parse import urlparse

from ..scrapers.link_scraper import LinkScraper
from ..database.db_handler import Database

logger = logging.getLogger(__name__)

class AutoCollector:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ø±ÙˆØ§Ø¨Ø·"""
    
    def __init__(self, whatsapp_client, database_handler: Database = None):
        """ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
        self.client = whatsapp_client
        self.db = database_handler
        self.is_collecting = False
        self.collection_tasks = []
        self.link_scraper = LinkScraper(self.db) if self.db else LinkScraper()
        self.collection_stats = {
            'total_links': 0,
            'last_collection': None,
            'collections_today': 0
        }
        self.collected_urls: Set[str] = set()
        self.collection_interval = 300  # 5 Ø¯Ù‚Ø§Ø¦Ù‚
        self.max_links_per_session = 10000
        self.last_group_check = {}
        
        logger.info("ğŸ“¥ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ")
    
    async def start_auto_collection(self, interval: int = None) -> Dict[str, Any]:
        """Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
        try:
            if self.is_collecting:
                logger.warning("âš ï¸ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙŠØ¹Ù…Ù„ Ø¨Ø§Ù„ÙØ¹Ù„")
                return {'success': False, 'error': 'Ø§Ù„ØªØ¬Ù…ÙŠØ¹ ÙŠØ¹Ù…Ù„ Ø¨Ø§Ù„ÙØ¹Ù„'}
            
            if not self.client.is_connected:
                logger.error("âŒ Ø§Ù„Ø¹Ù…ÙŠÙ„ ØºÙŠØ± Ù…ØªØµÙ„")
                return {'success': False, 'error': 'Ø§Ù„Ø¹Ù…ÙŠÙ„ ØºÙŠØ± Ù…ØªØµÙ„'}
            
            self.is_collecting = True
            
            # ØªØ¹ÙŠÙŠÙ† Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ø²Ù…Ù†ÙŠØ©
            collection_interval = interval or self.collection_interval
            
            logger.info(f"ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ (ÙØªØ±Ø©: {collection_interval} Ø«Ø§Ù†ÙŠØ©)")
            
            # Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
            collection_task = asyncio.create_task(
                self._collection_loop(collection_interval)
            )
            self.collection_tasks.append(collection_task)
            
            return {
                'success': True,
                'message': f'ØªÙ… Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ',
                'interval': collection_interval
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _collection_loop(self, interval: int):
        """Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
        try:
            while self.is_collecting:
                try:
                    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠÙˆÙ… Ø¬Ø¯ÙŠØ¯
                    await self._reset_daily_counter_if_needed()
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ
                    if self.collection_stats['collections_today'] >= 50:  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 50 Ø¹Ù…Ù„ÙŠØ© Ø¬Ù…Ø¹ ÙŠÙˆÙ…ÙŠÙ‹Ø§
                        logger.warning("âš ï¸ ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ø§Ù„ÙŠÙˆÙ…ÙŠ Ù„Ù„ØªØ¬Ù…ÙŠØ¹")
                        await asyncio.sleep(3600)  # Ø§Ù†ØªØ¸Ø§Ø± Ø³Ø§Ø¹Ø©
                        continue
                    
                    # Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¬Ù…ÙŠØ¹
                    await self._collect_from_all_groups()
                    
                    # Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙŠÙˆÙ…
                    self.collection_stats['collections_today'] += 1
                    
                    # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
                    logger.debug(f"â³ Ø§Ù†ØªØ¸Ø§Ø± {interval} Ø«Ø§Ù†ÙŠØ© Ù„Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªØ§Ù„ÙŠ")
                    await asyncio.sleep(interval)
                    
                except Exception as e:
                    logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¬Ù…ÙŠØ¹: {e}")
                    await asyncio.sleep(60)  # Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ù‚ÙŠÙ‚Ø© Ø«Ù… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {e}")
            self.is_collecting = False
    
    async def _reset_daily_counter_if_needed(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠÙˆÙ… Ø¬Ø¯ÙŠØ¯"""
        try:
            today = datetime.now().date()
            last_collection_date = self.collection_stats.get('last_collection_date')
            
            if last_collection_date != today:
                self.collection_stats['collections_today'] = 0
                self.collection_stats['last_collection_date'] = today
                logger.debug("ğŸ”„ ØªÙ… Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ÙŠÙˆÙ…ÙŠ")
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ: {e}")
    
    async def _collect_from_all_groups(self):
        """Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª"""
        try:
            logger.info("ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª...")
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
            from ..whatsapp.group_manager import GroupManager
            group_manager = GroupManager(self.client, self.db)
            groups = await group_manager.get_all_groups()
            
            if not groups:
                logger.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…ØªØ§Ø­Ø© Ù„Ù„ØªØ¬Ù…ÙŠØ¹")
                return
            
            total_links_collected = 0
            
            for group in groups:
                if not self.is_collecting:
                    break
                
                try:
                    group_id = group['id']
                    group_name = group['name']
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆÙ‚Øª Ø¢Ø®Ø± ÙØ­Øµ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
                    last_check = self.last_group_check.get(group_id)
                    if last_check:
                        time_since_last = datetime.now() - last_check
                        if time_since_last.total_seconds() < 3600:  # Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
                            logger.debug(f"â­ï¸ ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© {group_name} (ØªÙ… ÙØ­ØµÙ‡Ø§ Ù…Ø¤Ø®Ø±Ù‹Ø§)")
                            continue
                    
                    logger.info(f"ğŸ“‹ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©: {group_name}")
                    
                    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
                    from ..whatsapp.message_handler import MessageHandler
                    message_handler = MessageHandler(self.client, self.db)
                    
                    messages = await message_handler.get_group_messages(
                        group_id,
                        include_old=False,  # ÙÙ‚Ø· Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù†Ø° Ø¢Ø®Ø± ÙØ­Øµ
                        limit=100  # Ø¢Ø®Ø± 100 Ø±Ø³Ø§Ù„Ø© ÙÙ‚Ø·
                    )
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
                    links_collected = await self._process_group_messages(
                        messages, 
                        group_id, 
                        group_name
                    )
                    
                    total_links_collected += links_collected
                    
                    # ØªØ­Ø¯ÙŠØ« ÙˆÙ‚Øª Ø¢Ø®Ø± ÙØ­Øµ
                    self.last_group_check[group_id] = datetime.now()
                    
                    # Ø§Ù†ØªØ¸Ø§Ø± Ù‚ØµÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
                    await asyncio.sleep(2)
                    
                except Exception as e:
                    logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© {group.get('name')}: {e}")
                    continue
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.collection_stats['total_links'] += total_links_collected
            self.collection_stats['last_collection'] = datetime.now().isoformat()
            
            logger.info(f"âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ¬Ù…ÙŠØ¹: {total_links_collected} Ø±Ø§Ø¨Ø· Ø¬Ø¯ÙŠØ¯")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª: {e}")
    
    async def _process_group_messages(self, messages: List[Dict[str, Any]], 
                                     group_id: str, group_name: str) -> int:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"""
        try:
            links_collected = 0
            
            for message in messages:
                try:
                    message_text = message.get('body', '')
                    message_id = message.get('id', '')
                    sender = message.get('sender', '')
                    
                    if not message_text:
                        continue
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù†Øµ
                    all_links = self.link_scraper.extract_links(message_text)
                    
                    for link in all_links:
                        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ù… ØªÙƒØ±Ø§Ø± Ø§Ù„Ø±Ø§Ø¨Ø·
                        if link in self.collected_urls:
                            continue
                        
                        # ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ø§Ø¨Ø·
                        category = self.link_scraper.categorize_link(link)
                        
                        # Ø­ÙØ¸ Ø§Ù„Ø±Ø§Ø¨Ø·
                        if self.db:
                            success = await self.db.save_link({
                                'session_id': self.client.session_id if hasattr(self.client, 'session_id') else 'unknown',
                                'url': link,
                                'found_in': f"{group_name} - {sender}",
                                'group_id': group_id,
                                'message_id': message_id,
                                'category': category,
                                'metadata': {
                                    'group_name': group_name,
                                    'sender': sender,
                                    'message_preview': message_text[:100]
                                }
                            })
                            
                            if success:
                                links_collected += 1
                                self.collected_urls.add(link)
                                logger.debug(f"ğŸ”— ØªÙ… Ø­ÙØ¸ Ø±Ø§Ø¨Ø·: {link[:50]}...")
                        
                except Exception as e:
                    logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³Ø§Ù„Ø©: {e}")
                    continue
            
            return links_collected
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©: {e}")
            return 0
    
    async def stop_auto_collection(self) -> bool:
        """Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
        try:
            if not self.is_collecting:
                return True
            
            self.is_collecting = False
            
            # Ø¥Ù„ØºØ§Ø¡ Ø¬Ù…ÙŠØ¹ Ù…Ù‡Ø§Ù… Ø§Ù„ØªØ¬Ù…ÙŠØ¹
            for task in self.collection_tasks:
                if not task.done():
                    task.cancel()
                    try:
                        await task
                    except asyncio.CancelledError:
                        pass
            
            self.collection_tasks.clear()
            
            logger.info("â¹ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¬Ù…ÙŠØ¹: {e}")
            return False
    
    async def get_collection_status(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¬Ù…ÙŠØ¹"""
        status = {
            'is_collecting': self.is_collecting,
            'total_links_collected': self.collection_stats['total_links'],
            'last_collection': self.collection_stats['last_collection'],
            'collections_today': self.collection_stats['collections_today'],
            'unique_urls': len(self.collected_urls),
            'groups_monitored': len(self.last_group_check),
            'collection_interval': self.collection_interval,
            'max_links_per_session': self.max_links_per_session
        }
        
        if self.is_collecting and self.collection_tasks:
            status['active_tasks'] = len([t for t in self.collection_tasks if not t.done()])
        
        return status
    
    async def collect_from_specific_group(self, group_id: str, limit: int = 200) -> Dict[str, Any]:
        """Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø­Ø¯Ø¯Ø©"""
        try:
            logger.info(f"ğŸ” Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø­Ø¯Ø¯Ø©: {group_id}")
            
            from ..whatsapp.message_handler import MessageHandler
            from ..whatsapp.group_manager import GroupManager
            
            message_handler = MessageHandler(self.client, self.db)
            group_manager = GroupManager(self.client, self.db)
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
            group_info = await group_manager.get_group_info(group_id)
            if not group_info:
                return {'success': False, 'error': 'Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©'}
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
            messages = await message_handler.get_group_messages(
                group_id,
                include_old=True,
                limit=limit
            )
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
            links_collected = await self._process_group_messages(
                messages, 
                group_id, 
                group_info.get('name', 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')
            )
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.collection_stats['total_links'] += links_collected
            
            return {
                'success': True,
                'group_name': group_info.get('name'),
                'messages_processed': len(messages),
                'links_collected': links_collected,
                'total_links_now': self.collection_stats['total_links']
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø­Ø¯Ø¯Ø©: {e}")
            return {'success': False, 'error': str(e)}
    
    async def clear_collected_urls(self) -> int:
        """Ù…Ø³Ø­ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¬Ù…Ø¹Ø© Ù…Ø¤Ù‚ØªÙ‹Ø§"""
        try:
            count = len(self.collected_urls)
            self.collected_urls.clear()
            
            logger.info(f"ğŸ§¹ ØªÙ… Ù…Ø³Ø­ {count} Ø±Ø§Ø¨Ø· Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©")
            return count
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø³Ø­ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·: {e}")
            return 0
    
    async def export_collected_links(self, format: str = 'txt') -> Optional[str]:
        """ØªØµØ¯ÙŠØ± Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©"""
        try:
            if not self.db:
                logger.error("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")
                return None
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            links = await self.db.get_links(
                session_id=self.client.session_id if hasattr(self.client, 'session_id') else 'unknown',
                limit=10000
            )
            
            if not links:
                logger.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø±ÙˆØ§Ø¨Ø· Ù„Ù„ØªØµØ¯ÙŠØ±")
                return None
            
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø©
            categorized_links = {}
            for link in links:
                category = link['category']
                if category not in categorized_links:
                    categorized_links[category] = []
                categorized_links[category].append(link['url'])
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            if format == 'txt':
                filename = f"collected_links_{timestamp}.txt"
                filepath = f"data/exports/{filename}"
                
                os.makedirs(os.path.dirname(filepath), exist_ok=True)
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(f"ğŸ“‹ Ø±ÙˆØ§Ø¨Ø· Ù…Ø¬Ù…Ø¹Ø© - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write("=" * 50 + "\n\n")
                    
                    for category, urls in categorized_links.items():
                        f.write(f"ğŸ“ {category.upper()} ({len(urls)} Ø±Ø§Ø¨Ø·):\n")
                        f.write("-" * 30 + "\n")
                        for url in urls:
                            f.write(f"{url}\n")
                        f.write("\n")
                
                logger.info(f"ğŸ“¤ ØªÙ… ØªØµØ¯ÙŠØ± {len(links)} Ø±Ø§Ø¨Ø· Ø¥Ù„Ù‰: {filepath}")
                return filepath
            
            elif format == 'json':
                filename = f"collected_links_{timestamp}.json"
                filepath = f"data/exports/{filename}"
                
                os.makedirs(os.path.dirname(filepath), exist_ok=True)
                
                export_data = {
                    'exported_at': datetime.now().isoformat(),
                    'total_links': len(links),
                    'categories': categorized_links
                }
                
                with open(filepath, 'w', encoding='utf-8') as f:
                    import json
                    json.dump(export_data, f, ensure_ascii=False, indent=2)
                
                logger.info(f"ğŸ“¤ ØªÙ… ØªØµØ¯ÙŠØ± {len(links)} Ø±Ø§Ø¨Ø· Ø¥Ù„Ù‰: {filepath}")
                return filepath
            
            else:
                logger.error(f"âŒ ØªÙ†Ø³ÙŠÙ‚ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {format}")
                return None
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØµØ¯ÙŠØ± Ø§Ù„Ø±ÙˆØ§Ø¨Ø·: {e}")
            return None
    
    async def get_link_statistics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"""
        try:
            if not self.db:
                return {
                    'memory_only': True,
                    'unique_urls': len(self.collected_urls),
                    'categories': {}
                }
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            links = await self.db.get_links(
                session_id=self.client.session_id if hasattr(self.client, 'session_id') else 'unknown',
                limit=10000
            )
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            total_links = len(links)
            
            # Ø§Ù„ØªØµÙ†ÙŠÙ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
            category_stats = {}
            for link in links:
                category = link['category']
                if category not in category_stats:
                    category_stats[category] = 0
                category_stats[category] += 1
            
            # Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
            today = datetime.now().date()
            today_links = 0
            for link in links:
                try:
                    found_at = datetime.fromisoformat(link['found_at'].replace('Z', '+00:00'))
                    if found_at.date() == today:
                        today_links += 1
                except:
                    continue
            
            return {
                'total_links': total_links,
                'today_links': today_links,
                'categories': category_stats,
                'unique_in_memory': len(self.collected_urls)
            }
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø±ÙˆØ§Ø¨Ø·: {e}")
            return {'total_links': 0, 'today_links': 0, 'categories': {}}
    
    async def search_links(self, keyword: str, category: str = None) -> List[Dict[str, Any]]:
        """Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©"""
        try:
            if not self.db:
                return []
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
            links = await self.db.get_links(
                session_id=self.client.session_id if hasattr(self.client, 'session_id') else 'unknown',
                category=category,
                limit=1000
            )
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            results = []
            keyword_lower = keyword.lower()
            
            for link in links:
                try:
                    url = link.get('url', '').lower()
                    title = link.get('title', '').lower()
                    description = link.get('description', '').lower()
                    found_in = link.get('found_in', '').lower()
                    
                    if (keyword_lower in url or 
                        keyword_lower in title or 
                        keyword_lower in description or
                        keyword_lower in found_in):
                        results.append(link)
                        
                except:
                    continue
            
            logger.info(f"ğŸ” ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results)} Ø±Ø§Ø¨Ø· Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù€ '{keyword}'")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø·: {e}")
            return []
